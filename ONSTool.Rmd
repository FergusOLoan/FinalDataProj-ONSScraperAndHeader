---
title: "ONS Automatic Scrape and Visualiser"
author: "Fergus O'Loan"
date: "22/05/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Remember to put in library calls and install packages


```{r}
library(httr)
library(tidyRSS)
library(jsonlite)
library(rvest)
library(stringr)
library(rlist)
library(tidyverse)

url <- "www.ons.gov.uk/releasecalendar?rss"

RSScollected <- tidyfeed(url)


RSSCollectedSelect <- RSScollected %>% 
  select(item_pub_date, item_title, item_link)


#Worked on these two steps seperately; find some way of feeding 

testpages <- RSSCollectedSelect$item_link

mydata <- lapply(testpages, function(file) {
  read_html(file) %>% 
  html_nodes (".tiles__title-h3") %>%
  html_nodes ("a") %>%
  html_attr("href")
})

#need to unlist data to apply the paste properly, and for functions later

unlisted <- unlist(mydata)


datasets <- unlisted[grepl("datasets", unlisted)]


link <- paste("https://www.ons.gov.uk",datasets, sep = "",collapse = NULL)

#don't be dumb, https:// needed to register as html

RollTruDatasets <- lapply(link, function(link) {
  read_html(link) %>% 
  html_nodes (".border-bottom--abbey-lg") %>%
  html_nodes ("a") %>%
  html_attr("href")
})


TruDatasets <- unlist(RollTruDatasets)

Dls <- TruDatasets[grepl(".xls", TruDatasets)]

Downloads <- paste("https://www.ons.gov.uk",Dls, sep = "",collapse = NULL)

Downloads

GET(Downloads[1], write_disk(tf <- tempfile(fileext = ".xlsx")))
df <- read_xlsx(tf)
df
```



